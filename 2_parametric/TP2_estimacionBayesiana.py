# THEORY
# For a determined class, we want to know its density p(x)
# We know it's gaussian, but we're missing the value of the parameter 'u'.
# We marginalize over 'u', so p(x) = int(p(x|u)*p(u))
# We know p(x|u), since we know the functional form of p(x) for each fixed u.
# We're missing p(u), AKA, the parameter's distribution
# We want to use our samples D to shed some light on the issue, so we
# consider p(u) to be a posteriori, (so p(u) becomes p(u|D)) and because
# of Bayes, p(u|D)=p(D|u)p(u)
# This is the end of the road, p(D|u) is a particular case of the density
# p(x) for a fixed u (because the samples D were generated by the same
# process we're trying to figure out its density) and p(u) is just our
# previous knowledge over u
# This can be solved with MC, or analitically.
# Thanks to Duda, we know the analitical solution when the class' density
# and the parameter's density are both gaussian.


# COMPUTATION (one repetition with n fixed)
# Generation:
# We pick the parameters for the real (gaussian) distribution of x
# We generate n samples
#
# Estimation:
# We pick the parameters for our initial (gaussian) model
# We use the samples and the initial model to compute our u estimate (using Duda's formulas)


# QUESTIONS
# - How can this method converge if ultimately there's error in ours
# samples, and we don't really know if the process we're trying to model
# fits it perfectly. I mean... What are we converging to?
# Link to what I previously thought of considering u to be a va
#
# - Preguntar si la fórmula del algoritmo online en el tex
# está bien
#
# - ¿Cómo se normaliza una distribución conseguida tomando muestras? ¿Se
# divide por una aproximación de la integral?


#CODE
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

mu = 0
sigma = 1

mu_0 = 0
sigma_0 = 10000

fig, axes = plt.subplots(2, 2, sharex=True)
axes = axes.flatten()
sizes = [10, 100, 1000, 2000]

for i in range(0, 4):
    print('Muestreando (Tamaño ' + str(sizes[i]) + ')...', end='', flush=True)
    n = sizes[i]
    estimations = []
    for j in range(0, 10_000):
        samples = np.random.normal(size=n) * sigma + mu
        term_prom = float(n * sigma_0**2) / float(n * sigma_0**2 + sigma**2)
        term_prior = float(sigma**2) / float(n * sigma_0**2 + sigma**2)
        u_n = term_prom * samples.mean() + term_prior * mu_0
        estimations.append(u_n)
    ax = axes[i]
    ax.hist(estimations, 50, normed=1) # normed normalizes area
    ax.set_title(n)
    print(' Ok')

print('Graficando... Ok')
plt.show()
